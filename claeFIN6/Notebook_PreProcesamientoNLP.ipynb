{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\darly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\darly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\darly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\darly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BNW28mJQ3PI4",
    "outputId": "480192aa-0885-4493-9236-13557802293c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Los', 'gatos', 'son', 'animales', 'muy', 'interesantes', '.', 'Me', 'encanta', 'ver', 'cómo', 'juegan', 'y', 'exploran', 'el', 'mundo', '.']\n",
      "Tokens Normalizados: ['los', 'gatos', 'son', 'animales', 'muy', 'interesantes', '.', 'me', 'encanta', 'ver', 'cómo', 'juegan', 'y', 'exploran', 'el', 'mundo', '.']\n",
      "Tokens sin Stopwords: ['gatos', 'animales', 'interesantes', '.', 'encanta', 'ver', 'cómo', 'juegan', 'exploran', 'mundo', '.']\n",
      "Tokens Lematizados: ['gatos', 'animales', 'interesantes', '.', 'encanta', 'ver', 'cómo', 'juegan', 'exploran', 'mundo', '.']\n",
      "Tokens Stemmed: ['gato', 'animal', 'interesant', '.', 'encanta', 'ver', 'cómo', 'juegan', 'exploran', 'mundo', '.']\n",
      "\n",
      "Bolsa de Palabras:\n",
      "[[1 1 1 1 1 1 1 1 1]]\n",
      "Características (BOW): ['animales' 'cómo' 'encanta' 'exploran' 'gatos' 'interesantes' 'juegan'\n",
      " 'mundo' 'ver']\n",
      "\n",
      "TF-IDF:\n",
      "[[0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      "  0.33333333 0.33333333 0.33333333]]\n",
      "Características (TF-IDF): ['animales' 'cómo' 'encanta' 'exploran' 'gatos' 'interesantes' 'juegan'\n",
      " 'mundo' 'ver']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de texto\n",
    "text = \"Los gatos son animales muy interesantes. Me encanta ver cómo juegan y exploran el mundo.\"\n",
    "\n",
    "# 1. Tokenización\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2. Normalización (conversión a minúsculas)\n",
    "tokens_normalized = [token.lower() for token in tokens]\n",
    "print(\"Tokens Normalizados:\", tokens_normalized)\n",
    "\n",
    "# 3. Eliminación de Stopwords\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "tokens_no_stopwords = [token for token in tokens_normalized if token not in stop_words]\n",
    "print(\"Tokens sin Stopwords:\", tokens_no_stopwords)\n",
    "\n",
    "# 4. Lematización\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens_lemmatized = [lemmatizer.lemmatize(token) for token in tokens_no_stopwords]\n",
    "print(\"Tokens Lematizados:\", tokens_lemmatized)\n",
    "\n",
    "# 5. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "tokens_stemmed = [stemmer.stem(token) for token in tokens_no_stopwords]\n",
    "print(\"Tokens Stemmed:\", tokens_stemmed)\n",
    "\n",
    "# Representación de Texto\n",
    "\n",
    "# 6. Bolsa de Palabras\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform([' '.join(tokens_no_stopwords)])\n",
    "print(\"\\nBolsa de Palabras:\")\n",
    "print(X_bow.toarray())\n",
    "print(\"Características (BOW):\", vectorizer_bow.get_feature_names_out())\n",
    "\n",
    "# 7. TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform([' '.join(tokens_no_stopwords)])\n",
    "print(\"\\nTF-IDF:\")\n",
    "print(X_tfidf.toarray())\n",
    "print(\"Características (TF-IDF):\", vectorizer_tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
